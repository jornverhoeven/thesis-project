\section{Discussion}
\label{sec:discussion}
Looking back at the results of the experiments, we can see that all feature sets are able to reduce the overall damage of the infrastructure. Where the local-agent (our baseline) is able to reduce a smaller amount of damage, the auctioning and knowledge-sharing agents are able to reduce the damage to an even lower level. This section will go into detail about the strengths and weaknesses of this implementation of the ADRIAN protocol, and therefore also discuss the strengths and weaknesses for the protocol itself. 

% \add{Mention the amount of risks detected}
\subsection{Risks Detected}
\label{ssec:risks-detected}
From the figures detailing the unique number of risks detected (figures \ref{fig:risk-count-no-change}, \ref{fig:risk-count-risk-introduction}, \ref{fig:risk-count-growing}, and \ref{fig:risk-count-unstable}) we can see that the local-agent is only able to detect a fraction of the risks that the auctioning and knowledge-sharing agents are able to detect. This is expected as the local-agent cannot share any information with its neighbors, and therefore cannot detect any risks that have an origin outside of its own infrastructure node and software component.

The knowledge-sharing agents are able to detect more risks than the auctioning agents, however this is assumed to be because of the fact that there are more adaptations applied by the knowledge-sharing agents (figures \ref{fig:proposals-no-change}, \ref{fig:proposals-risk-introduction}, \ref{fig:proposals-growing}, and \ref{fig:proposals-unstable}). This means that the same critical paths are found, each with changing probabilities and damage. This results in more \emph{unique} risks to be detected, as this tends to scale with the amount of adaptations applied. 

As mentioned before, in the ADRIAN-concept by Mann and Smolka \cite{mann2023ADRIAN} the distance a message can travel is detailed. We believe that the agents would be able to detect more risks if this distance was increased. However, metrics that are collected are not enough to conclude this fully. In the next section (Section \ref{sec:future-research}) this will be discussed further.

% \add{Mention that adaptation count is the same, but time spent adapting is different}
\subsection{Damage Reduction \& Adaptations}
\label{ssec:efficient-adaptations}
In the results we see that overall the knowledge-sharing and auctioning agents tend to perform similarly when looking at the overall damage of the infrastructure (figures \ref{fig:overall-damage-no-change}, \ref{fig:overall-damage-inroduce-risk}, \ref{fig:overall-damage-growing}, and \ref{fig:overall-damage-unstable}), with the auctioning agent usually getting just below the others. However, when looking at the number of adaptations that are applied (figures \ref{fig:proposals-no-change}, \ref{fig:proposals-risk-introduction}, \ref{fig:proposals-growing}, and \ref{fig:proposals-unstable}) and compare this to the damage reduction, we can see that the auctioning agent is able to reduce the damage with less adaptations. This is an indicator that the auctioning agents are able to apply more efficient adaptations, as they are able to reduce the same damage nearly the same amount, in with adaptations.

% \add{Explain why knowledge-sharing damage output is sometimes better, but more time adaptating is bad}
\subsection{Adaptation Time}
\label{ssec:adaptation-time}
When we look at the amount of adaptations (figures \ref{fig:proposals-no-change}, \ref{fig:proposals-risk-introduction}, \ref{fig:proposals-growing}, and \ref{fig:proposals-unstable}) and time spent adapting (figures \ref{fig:adapting-time-no-change}, \ref{fig:adapting-time-risk-introduction}, \ref{fig:adapting-time-growing}, and \ref{fig:adapting-time-unstable}), we can see that in all cases the knowledge-sharing agents spend more (up to twice as much) time applying adaptations.
Depending on the adaptation, this could mean that the software components running on the infrastructure node could become unavailable for longer periods of time, and that the infrastructure node is unresponsive while applying the adaptations.  In the last scenario (figure \ref{fig:adapting-time-unstable}) we see that the agents in the knowledge-sharing feature-set spend $112$ seconds of the $240$ seconds it was running, which is $46.7\%$ of the time it was running. The time spent auctioning is cumulative, so this percentage is not as high as it seems. However, compared to the auctioning node it is nearly twice as much time spent adapting.

In a real-world scenario this downtime would likely be unfavorable, as this downtime could mean users could be prevented from using the software. The \emph{cost} of this downtime should therefore be weighed against the damage that potentially could be done when not applying any adaptations. Applying many adaptations early on could prevent any damage later on, which could be more important than the downtime. 
In some real-world scenario it could be possible to pair certain adaptations, as downtime from some software component could lead to the indirect unavailability of another one. Therefore any adaptation that would lead to downtime for the latter component, could be done in parallel. This could be a beneficial strategy when downtime is a concern.

% \add{Mention the stability of the system}
\subsection{Stability}
\label{ssec:stability}
As the scenarios run their course, we can see that the knowledge-sharing and auctioning agents are able to keep the damage of the infrastructure at a steady level. Especially in the Growing Infrastructure scenario (figure \ref{fig:overall-damage-growing}) we see that the damage is kept at a relatively low level, even though the infrastructure is growing. This is a good indicator that the ADRIAN protocol is able to handle changes in the infrastructure, next to being able to reduce the overall damage of the infrastructure.

\subsection{Small Infrastructure Overhead}
During the implementation of the system we noticed that the performance of the different feature-sets was slightly different when the infrastructure was small. The knowledge-sharing agents tends to reach the lowest points almost twice as fast, compared to the auctioning agent. We believe this to be because of the overhead that is introduced by auctioning. Figure \ref{fig:small-infra-no-change} shows the performance of the different feature-sets in the no-change scenario with a small infrastructure consisting of four infrastructure nodes and two software components.

\begin{figure}[H]
    \centering
        \input{graphs/small-infra-no-change}
    \caption{Graph showing the output of all feature sets in the non-changing scenario, with a small infrastructure.}
    \label{fig:small-infra-no-change}
\end{figure}

After carefully inspecting the logs, two likely causes are identified. Instead of applying the adaptations directly, the agents first have to auction the adaptations. And after this time only one is allowed to apply the adaptation. This while the knowledge-sharing agents can apply these found adaptations directly. In this case the latter is considered to be a good thing, but as mentioned in subsection \ref{ssec:adaptation-time} this could also be considered a bad thing.

Additionally, because of the infrastructure size there is only a limited amount of risks, the changes of finding a better adaptation via an auction is smaller in that case. Both of these factors are likely to cause this difference in performance, but further research would be needed to confirm this.

% \subsection{Large Infrastructure}
% \add{rerun}
% \begin{figure}[H]
%     \centering
%         \input{graphs/large-infra-large}
%     \caption{Graph showing the output of all feature sets in the non-changing scenario, with a large infrastructure.}
%     \label{fig:large-infra-large}
% \end{figure}

% \add{Mention the spread in consecutive runs}
% \add{Mention the performance of knowledge-sharing in smaller networks}

\subsection{Consistency}
\label{ssec:consecutive-runs}
During some of the experiments we see a small fluctuation in the behavior of agents. One good example of this is figure \ref{fig:adapting-time-no-change} compared to \ref{fig:adapting-time-risk-introduction}. We believe this is because of the scheduling of threads by the OS. This could lead into different auctions being initiated and joined over multiple runs. 
 
To test this assumption, we ran the no-change scenario with the auctioning feature-set $5$ times. The results of this can be seen in figure \ref{fig:multi-run-no-change}. We see that the overall damage is very similar in all runs. However, small differences can be observed across the runs.

\begin{figure}[H]
    \centering
        \input{graphs/multi-run-no-change}
    \caption{Graph showing the Auctioning Feature's performance over multiple runs in the no-change scenario.}
    \label{fig:multi-run-no-change}
\end{figure}

The metrics that are collected are not enough to fully pin-point this behavior to scheduling. But, the results seem to indicate that the variety in the performance of the agents is small, and no further investigation has been performed. However, The result from figure \ref{fig:multi-run-no-change} are positive, as it means that the agents are able to perform consistently over multiple runs.
